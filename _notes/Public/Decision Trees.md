---
title: Decision Trees
feed: show
date : 2023-04-14
tags : [📝️/🌲️, ML]
comments: true
---

# 1. Introduction
### 1.1 ✂️TL;DR
예측이나 분류를 위해 결정과 그에 따른 결과들을 `트리 구조`로 표현한 알고리즘

### 1.2 🎓Paper summary
- [[Induction of Decision Trees summary]]

### 1.3 🔗Useful links
- [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning)

# 2. How It Works
### 2.1 🔑Key 

> 의사결정 프로세스와 그에 따른 결과를 트리 구조로 표현한 모델로 분류 또는 회귀작업에 사용

- 의사결정나무는 직관적이고 해석하기 쉬운 모델로써, 변수들 간의 상호작용과 영향력을 이해하는 데 도움을 준다.
- 가지치기, 가중치, 엔트로피 등 다양한 분할 기준과 알고리즘을 사용하여 모델의 성능과 일반화 능력을 향상시킬 수 있다.
- 과적합 방지를 위한 가지치기 및 사전 가지치기 등의 기법을 적용하여 모델의 일반화 성능을 향상시킬 수 있다.

### 2.2 ⛓️Steps 
1. 데이터 분할: 주어진 데이터셋을 특성 값을 기준으로 두 개의 하위 집단으로 분할한다. (각 분할은 데이터를 가장 잘 분류하는 기준이 되는 특성을 선택한다.)
2. 분할 기준 선택: 엔트로피 또는 지니 지수와 같은 평가 척도를 사용하여 최적의 분할 기준을 선택한다. (분할 기준은 정보 이득이 최대화되거나 불순도가 최소화되는 방향으로 결정된다.)
3. 재귀적 분할: 분할된 하위 데이터셋에 대해 동일한 프로세스를 재귀적으로 반복한다. (각 하위 데이터셋은 더 작은 데이터 그룹으로 분할되며, 분할은 불순도 감소를 목표로 진행된다.)
4. 종료 조건: 분할 과정은 일정한 깊이, 노드 수, 또는 불순도 감소와 같은 종료 조건을 만족할 때까지 진행한다. (각 하위 집합에 대해 리프 노드(leaf node)를 생성합니다.)
5. 예측: 생성된 의사결정 규칙을 사용하여 새로운 데이터에 대한 분류 또는 예측을 수행한다.

![](/attachments/Pasted_image_20230522072009_watermarked.jpeg)

(\### image from [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning))

# 3. Pros and Cons
### 3.1 Pros
- 의사결정나무는 비교적 이해하기 쉽고 해석하기 쉽다.
- 분류와 회귀 작업 모두에 사용할 수 있다. 
- 다양한 데이터 유형에 대해 학습할 수 있다. 
- 학습 및 예측이 상대적으로 효율적이다.

### 3.2 Cons
- 과적합에 민감할 수 있다.
- 대규모 데이터셋에 확장하기 어려울 수 있다.
- 학습 데이터에 편향될 수 있다.

# 4. Application
1. **앙상블**: 여러 개의 의사결정나무를 결합하여 더 강력한 모델을 만든다. 
	- 예를 들어, 랜덤 포레스트는 의사결정나무의 모음을 구축하고 그 예측을 집계하는 방식으로 작동한다. 
	- 그래디언트 부스팅은 이전에 잘못 분류된 샘플에 초점을 맞추어 의사결정나무를 순차적으로 구축한다.
2. **가지치기**: 의사결정나무의 복잡성을 줄이기 위해 불필요한 가지를 제거하는 기법이다. 이는 과적합을 방지하고 모델의 일반화 능력을 향상시킨다.
3. **특성(속성) 선택**: 관련 없거나 중복된 특성에 민감할 수 있다. 특성 선택 기법은 의사결정나무 구축에 가장 유용한 특성을 식별하여 성능과 해석 가능성을 향상시킨다.
4. **규칙 유도**: 규칙 유도 알고리즘은 의사결정나무로부터 규칙을 추출하는 것을 목표로 한다. 이를 통해 모델의 이해와 해석이 용이해집니다. (설명가능한 모델)
5. **의사결정나무의 변형**: ID3, C4.5, CART, CHAID와 같은 다양한 의사결정나무의 변형이 있으며, 각각의 특징과 알고리즘을 가지고 있다. (분할 기준, 결측값 처리 또는 가지치기 전략 등을 다르게 적용)
6. **불균형 데이터 처리**: 의사결정나무는 한 클래스가 다른 클래스보다 훨씬 많은 인스턴스를 가지는 불균형한 데이터에 영향을 받을 수 있다. 이는 클래스 가중치, 언더샘플링, 오버샘플링과 같은 기법을 사용하여 이 문제를 해결할 수 있다.

_끝_