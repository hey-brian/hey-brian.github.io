---
title: List
feed: hide
date: 2022-12-06
permalink: /list
---

## Mathematics
### [[Bootstrap]] #statistics
---
- [Problems in Plane Sampling(1949)](https://projecteuclid.org/euclid.aoms/1177729989) #ProjectEuclid _by_ Quenouille, 
	- 📝[[Problems in Plane Sampling summary]]
- [Notes on Bias Estimation(1958)](https://www.jstor.org/stable/2332914?seq=1) #JSTOR _by_ Quenouille,  
	- 📝[[Notes on Bias Estimation summary]]
- [Bias and Confidence in Not-quite Large Samples(1958)](https://www.jstor.org/stable/2332914?seq=1) #JSTOR _by_ Tukey, 
	- 📝[[Bias and Confidence in Not-quite Large Samples summary]]
- [Bootstrap Methods: Another Look at the Jackknife(1979)](https://projecteuclid.org/euclid.aos/1176344552) #ProjectEuclid _by_ Efron, 
	- 📝[[Bootstrap Methods_Another Look at the Jackknife summary]]

### [[Lasso]] #statistics #regression
---
- [Linear Inversion of Band-Limited Reflection Seismograms(1986)](https://epubs.siam.org/doi/10.1137/0907087) #epubs _by_ Santosa and Symes, 
	- 📝[[Linear Inversion of Band-Limited Reflection Seismograms summary]]
- [Regression Shrinkage and Selection Via the Lasso(1994)](https://www.jstor.org/stable/2346178) #JSTOR _by_ Tibshirani, 
	- 📝[[Regression Shrinkage and Selection Via the Lasso summary]]

### [[Elastic Net]] #statistics #regression
---
- [Regularization and variable selection via the Elastic Net(2005)](https://www.jstor.org/stable/3647580) #JSTOR _by_ Zou and Hastie, 
	- 📝[[Regularization and variable selection via the Elastic Net summary]]

### [[Gradient Descent]] #optimization

### [[Stochastic Gradient Descent]] #optimization
---
- [A Stochastic Approximation Method(1951)](https://projecteuclid.org/euclid.aoms/1177729586) #ProjectEuclid _by_ Robbins and Monro, 
	- 📝[[A Stochastic Approximation Method summary]]
- [Stochastic Estimation of the Maximum of a Regression Function(1952)](https://projecteuclid.org/euclid.aoms/1177729392) #ProjectEuclid _by_ Kiefer and Wolfowitz, 
	- 📝[[Stochastic Estimation of the Maximum of a Regression Function summary]]

### [[Expectation Maximization]] #optimization 
---
- [Maximum likelihood from incomplete data via the EM algorithm(1977)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.133.4884) #CiteSeerX _by_ Dempster, Laird, and Rubin, 
	- 📝[[Maximum likelihood from incomplete data via the EM algorithm summary]]

### [[Adam]] #optimization 
---
- [Adam: A Method for Stochastic Optimization(2015)](https://arxiv.org/abs/1412.6980) #arXiv _by_ Kingma and Ba, 
	- 📝[[Adam_A Method for Stochastic Optimization summary]]

### [[Autograd]]
---
- [Autograd: Effortless Gratients in Numpy(2015)](https://indico.ijclab.in2p3.fr/event/2914/contributions/6483/subcontributions/180/attachments/6060/7185/automl-short.pdf)  #ICML _by_ Dougal Maclaurin, David Duvenaud, Ryan P. Adams 
	- 📝[[Autograd_Effortless Gratients in Numpy summary]]
	- 🖋️[Github](https://github.com/HIPS/autograd)

### [[Backpropagation]] #training 
---
- [Learning representations by back-propagating errors(1986)](https://www.nature.com/articles/323533a0) #Nature _by_ Rumelhart, Hinton, and Williams, 
	- 📝[[Learning representations by back-propagating errors summary]]
- [Backpropagation Applied to Handwritten Zip Code Recognition(1989)](https://ieeexplore.ieee.org/document/6795724) #IEEE _by_ LeCun et al., 
	- 📝[[Backpropagation Applied to Handwritten Zip Code Recognition summary]]
	- 🖋️[Author's note](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)

## Modeling
### [[Bagging]] #ensemble
--- 
- [Bagging Predictors(1996)](https://link.springer.com/article/10.1023/A:1018054314350) #Springer _by_ Breiman,
	- 📝[[Bagging Predictors summary]]

### [[AdaBoost]] ([[Boosting]]) #ensemble
---
- [A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting(1997—published as abstract in 1995)](https://link.springer.com/chapter/10.1007/3-540-59119-2_166) #Springer _by_ Freund and Schapire,  
	- 📝[[A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting summary]]
- [Experiments with a New Boosting Algorithm(1996)](https://citeseerx.ist.psu.edu/search_result?query=Experiments+with+a+New+Boosting+Algorithm+%281996%29%2C+Freund+and+Schapire&pdf=true) #CiteSeerX _by_ Freund and Schapire, 
	- 📝[[Experiments with a New Boosting Algorithm summary]]

### [[Gradient Boosting]] #ensemble 
---
- [Greedy function approximation: A gradient boosting machine(2001)](https://projecteuclid.org/euclid.aos/1013203451) #ProjectEuclid _by_ Friedman, 
	- 📝[[Greedy function approximation_A gradient boosting machine summary]]
- [XGBoost: A Scalable Tree Boosting System(2016)](https://arxiv.org/abs/1603.02754) #arXiv _by_ Chen and Guestrin, 
	- 📝[[XGBoost_A Scalable Tree Boosting System summary]]

### [[Dropout]]
---
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting(2014)](http://jmlr.org/papers/v15/srivastava14a.html) #JMLR _by_ Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov, 
	- 📝[[Dropout_A Simple Way to Prevent Neural Networks from Overfitting summary]]

## Machine Learning
### [[Perceptron]]
---
- [The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain(1958)](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.588.3775) #CiteSeerX _by_ Rosenblatt, 
	- 📝[[The Perceptron_A Probabilistic Model for Information Storage and Organization in The Brain summary]]

### [[Decision Trees]] #supervised
---
- [Induction of Decision Trees(1986)](https://link.springer.com/article/10.1007/BF00116251) #Springer _by_ Quinlan, 
	- 📝[[Induction of Decision Trees summary]]

### [[k-Nearest Neighbors]] #supervised
---
- [Nearest neighbor pattern classification(1967)](https://ieeexplore.ieee.org/abstract/document/1053964) #IEEE _by_ Cover and Hart, 
	- 📝[[Nearest neighbor pattern classification summary]]
- [E. Fix and J.L. Hodges(1951): An Important Contribution to Nonparametric Discriminant Analysis and Density Estimation(1989)](https://www.jstor.org/stable/1403796?seq=1) #JSTOR _by_ Silverman and Jones, 
	- 📝[[An Important Contribution to Nonparametric Discriminant Analysis and Density Estimation summary]]

### [[Latent Dirichlet Allocation]] #NLP
---
- [Latent Dirichlet Allocation(2003)](http://jmlr.csail.mit.edu/papers/v3/blei03a.html) #JMLR _by_ Blei, Ng, and Jordan, 
	- 📝[[Latent Dirichlet Allocation summary]]

### [[Random Forest]] #supervised
---
- [Random Forests(2001)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.5395) #CiteSeerX _by_ Breiman and Schapire, 
	- 📝[[Random Forests summary]]

### [[Support Vector Machine]] #supervised
---
- [Support Vector Networks(1995)](https://link.springer.com/article/10.1023/A:1022627411411) #Springer _by_ Cortes and Vapnik, 
	- 📝[[Support Vector Networks summary]]

### [[Generative Adversarial Networks]] #generative
---
- [General Adversarial Nets(2014)](https://papers.nips.cc/paper/5423-generative-adversarial-nets) #nips _by_ Goodfellow et al., 
	- 📝[[General Adversarial Nets summary]]

## Programming
### Python
---
- 📝[[pandas(판다스) 쿼리(query) 조회]]

### Visualization
---
- 📝[[Scikit-Mobility(skmob) tutorial]]

## ETC
--- 
- 📝[[iphone url scheme 정리]]
