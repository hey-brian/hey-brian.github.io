---
title: List
feed: hide
date: 2022-12-06
permalink: /list
---

## Mathematics
### [[Bootstrap]] #statistics
---
- [Problems in Plane Sampling(1949)](https://projecteuclid.org/euclid.aoms/1177729989) #ProjectEuclid _by_ Quenouille,Â 
	- ğŸ“[[Problems in Plane Sampling summary]]
- [Notes on Bias Estimation(1958)](https://www.jstor.org/stable/2332914?seq=1) #JSTOR _by_ Quenouille,Â  
	- ğŸ“[[Notes on Bias Estimation summary]]
- [Bias and Confidence in Not-quite Large Samples(1958)](https://www.jstor.org/stable/2332914?seq=1) #JSTOR _by_ Tukey, 
	- ğŸ“[[Bias and Confidence in Not-quite Large Samples summary]]
- [Bootstrap Methods: Another Look at the Jackknife(1979)](https://projecteuclid.org/euclid.aos/1176344552) #ProjectEuclid _by_ Efron,Â 
	- ğŸ“[[Bootstrap Methods_Another Look at the Jackknife summary]]

### [[Lasso]] #statistics #regression
---
- [Linear Inversion of Band-Limited Reflection Seismograms(1986)](https://epubs.siam.org/doi/10.1137/0907087) #epubs _by_ Santosa and Symes,Â 
	- ğŸ“[[Linear Inversion of Band-Limited Reflection Seismograms summary]]
- [Regression Shrinkage and Selection Via the Lasso(1994)](https://www.jstor.org/stable/2346178) #JSTOR _by_ Tibshirani, 
	- ğŸ“[[Regression Shrinkage and Selection Via the Lasso summary]]

### [[Elastic Net]] #statistics #regression
---
- [Regularization and variable selection via the Elastic Net(2005)](https://www.jstor.org/stable/3647580)Â #JSTOR _by_ Zou and Hastie, 
	- ğŸ“[[Regularization and variable selection via the Elastic Net summary]]

### [[Gradient Descent]] #optimization

### [[Stochastic Gradient Descent]] #optimization
---
- [A Stochastic Approximation Method(1951)](https://projecteuclid.org/euclid.aoms/1177729586) #ProjectEuclid _by_ Robbins and Monro,Â 
	- ğŸ“[[A Stochastic Approximation Method summary]]
- [Stochastic Estimation of the Maximum of a Regression Function(1952)](https://projecteuclid.org/euclid.aoms/1177729392) #ProjectEuclid _by_ Kiefer and Wolfowitz, 
	- ğŸ“[[Stochastic Estimation of the Maximum of a Regression Function summary]]

### [[Expectation Maximization]] #optimization 
---
- [Maximum likelihood from incomplete data via the EM algorithm(1977)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.133.4884) #CiteSeerX _by_ Dempster, Laird, and Rubin, 
	- ğŸ“[[Maximum likelihood from incomplete data via the EM algorithm summary]]

### [[Adam]] #optimization 
---
- [Adam: A Method for Stochastic Optimization(2015)](https://arxiv.org/abs/1412.6980) #arXiv _by_ Kingma and Ba, 
	- ğŸ“[[Adam_A Method for Stochastic Optimization summary]]

### [[Autograd]]
---
- [Autograd: Effortless Gratients in Numpy(2015)](https://indico.ijclab.in2p3.fr/event/2914/contributions/6483/subcontributions/180/attachments/6060/7185/automl-short.pdf)  #ICML _by_ Dougal Maclaurin, David Duvenaud, Ryan P. Adams 
	- ğŸ“[[Autograd_Effortless Gratients in Numpy summary]]
	- ğŸ–‹ï¸[Github](https://github.com/HIPS/autograd)

### [[Backpropagation]] #training 
---
- [Learning representations by back-propagating errors(1986)](https://www.nature.com/articles/323533a0) #Nature _by_ Rumelhart, Hinton, and Williams,Â 
	- ğŸ“[[Learning representations by back-propagating errors summary]]
- [Backpropagation Applied to Handwritten Zip Code Recognition(1989)](https://ieeexplore.ieee.org/document/6795724) #IEEE _by_ LeCun et al.,Â 
	- ğŸ“[[Backpropagation Applied to Handwritten Zip Code Recognition summary]]
	- ğŸ–‹ï¸[Author's note](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)

## Modeling
### [[Bagging]] #ensemble
--- 
- [Bagging Predictors(1996)](https://link.springer.com/article/10.1023/A:1018054314350) #Springer _by_ Breiman,
	- ğŸ“[[Bagging Predictors summary]]

### [[AdaBoost]] ([[Boosting]]) #ensemble
---
- [A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting(1997â€”published as abstract in 1995)](https://link.springer.com/chapter/10.1007/3-540-59119-2_166) #Springer _by_ Freund and Schapire,Â  
	- ğŸ“[[A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting summary]]
- [Experiments with a New Boosting Algorithm(1996)](https://citeseerx.ist.psu.edu/search_result?query=Experiments+with+a+New+Boosting+Algorithm+%281996%29%2C+Freund+and+Schapire&pdf=true) #CiteSeerX _by_ Freund and Schapire, 
	- ğŸ“[[Experiments with a New Boosting Algorithm summary]]

### [[Gradient Boosting]] #ensemble 
---
- [Greedy function approximation: A gradient boosting machine(2001)](https://projecteuclid.org/euclid.aos/1013203451) #ProjectEuclid _by_ Friedman, 
	- ğŸ“[[Greedy function approximation_A gradient boosting machine summary]]
- [XGBoost: A Scalable Tree Boosting System(2016)](https://arxiv.org/abs/1603.02754) #arXiv _by_ Chen and Guestrin,Â 
	- ğŸ“[[XGBoost_A Scalable Tree Boosting System summary]]

### [[Dropout]]
---
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting(2014)](http://jmlr.org/papers/v15/srivastava14a.html) #JMLR _by_ Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov, 
	- ğŸ“[[Dropout_A Simple Way to Prevent Neural Networks from Overfitting summary]]

## Machine Learning
### [[Perceptron]]
---
- [The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain(1958)](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.588.3775) #CiteSeerX _by_ Rosenblatt,Â 
	- ğŸ“[[The Perceptron_A Probabilistic Model for Information Storage and Organization in The Brain summary]]

### [[Decision Trees]] #supervised
---
- [Induction of Decision Trees(1986)](https://link.springer.com/article/10.1007/BF00116251)Â #Springer _by_ Quinlan, 
	- ğŸ“[[Induction of Decision Trees summary]]

### [[k-Nearest Neighbors]] #supervised
---
- [Nearest neighbor pattern classification(1967)](https://ieeexplore.ieee.org/abstract/document/1053964) #IEEE _by_ Cover and Hart, 
	- ğŸ“[[Nearest neighbor pattern classification summary]]
- [E. Fix and J.L. Hodges(1951): An Important Contribution to Nonparametric Discriminant Analysis and Density Estimation(1989)](https://www.jstor.org/stable/1403796?seq=1) #JSTOR _by_ Silverman and Jones, 
	- ğŸ“[[An Important Contribution to Nonparametric Discriminant Analysis and Density Estimation summary]]

### [[Latent Dirichlet Allocation]] #NLP
---
- [Latent Dirichlet Allocation(2003)](http://jmlr.csail.mit.edu/papers/v3/blei03a.html) #JMLR _by_ Blei, Ng, and Jordan, 
	- ğŸ“[[Latent Dirichlet Allocation summary]]

### [[Random Forest]] #supervised
---
- [Random Forests(2001)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.5395)Â #CiteSeerX _by_ Breiman and Schapire, 
	- ğŸ“[[Random Forests summary]]

### [[Support Vector Machine]] #supervised
---
- [Support Vector Networks(1995)](https://link.springer.com/article/10.1023/A:1022627411411) #Springer _by_ Cortes and Vapnik,Â 
	- ğŸ“[[Support Vector Networks summary]]

### [[Generative Adversarial Networks]] #generative
---
- [General Adversarial Nets(2014)](https://papers.nips.cc/paper/5423-generative-adversarial-nets) #nips _by_ Goodfellow et al., 
	- ğŸ“[[General Adversarial Nets summary]]

## Programming
### Python
---
- ğŸ“[[pandas(íŒë‹¤ìŠ¤) ì¿¼ë¦¬(query) ì¡°íšŒ]]

### Visualization
---
- ğŸ“[[Scikit-Mobility(skmob) tutorial]]

## ETC
--- 
- ğŸ“[[iphone url scheme ì •ë¦¬]]
