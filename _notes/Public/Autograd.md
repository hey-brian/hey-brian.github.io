---
title: Autograd
feed: show
date : 2023-06-13
time : 07:38:03
tags : []
format: list
comments: true
---

# 1. Introduction
### 1.1 β‚οΈTL;DR
- Autogradλ” μλ™ λ―Έλ¶„(automatic differentiation)μ„ μ κ³µν•λ” λΌμ΄λΈλ¬λ¦¬λ΅, λ”¥ λ¬λ‹μ—μ„ κ·Έλλ””μ–ΈνΈ κ³„μ‚°μ„ μλ™μΌλ΅ μ²λ¦¬ν•λ” κΈ°λ¥μ„ μ κ³µν•λ‹¤.

### 1.2 π“Paper summary
- [[Autograd_Effortless Gratients in Numpy summary]]

### 1.3 π”—Useful links
- [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation)


# 2. How It Works
### 2.1 π”‘Key 
> Backpropagation κ³„μ‚°μ„ μ„ν•΄ ν•„μ”ν• gradient(κ·Έλλ””μ–ΈνΈ)λ¥Ό μλ™μΌλ΅ κ³„μ‚°

### 2.2 β›“οΈSteps 
1. **κ³„μ‚° κ·Έλν”„ κµ¬μ„±**: μ‚¬μ©μκ°€ μ •μν• μ—°μ‚°λ“¤λ΅ κ³„μ‚° κ·Έλν”„λ¥Ό κµ¬μ„±ν•λ‹¤.
2. **μ „μ§„ ν¨μ¤**: μ…λ ¥ λ°μ΄ν„°λ¥Ό κ³„μ‚° κ·Έλν”„μ— μ „λ‹¬ν•μ—¬ μ¤‘κ°„ κ²°κ³Όλ¥Ό κ³„μ‚°ν•λ‹¤.
3. **μ—­μ „ν¨μ¤ (Backpropagation)**: μ „μ§„ ν¨μ¤μ—μ„ κµ¬ν• κ²°κ³Όλ¥Ό λ°”νƒ•μΌλ΅ κ·Έλλ””μ–ΈνΈλ¥Ό μ—­μ „ν¨μ¤ν•μ—¬ κ° λ³€μμ— λ€ν• λ―Έλ¶„ κ°’μ„ κ³„μ‚°ν•λ‹¤.
4. **κ·Έλλ””μ–ΈνΈ μ €μ¥**(update): κ³„μ‚°λ κ·Έλλ””μ–ΈνΈλ¥Ό κ° λ³€μμ— μ €μ¥ν•μ—¬ ν›„μ† μ—°μ‚°μ— ν™μ©ν•λ‹¤.

![](/attachments/Pasted_image_20230619071141_watermarked.jpeg)

(\### image from [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation))

![](/attachments/Pasted_image_20230619071304_watermarked.jpeg)

(\### image from [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation))


# 3. Pros and Cons
### 3.1 π„Pros
- μ‚¬μ©μκ°€ μ§μ ‘ κ·Έλλ””μ–ΈνΈλ¥Ό κ³„μ‚°ν•λ” λ²κ±°λ΅μ›€μ„ μ¤„μ—¬μ¤€λ‹¤.
- λ³µμ΅ν• μ—°μ‚°μ κ·Έλλ””μ–ΈνΈλ¥Ό ν¨κ³Όμ μΌλ΅ κ³„μ‚°ν•  μ μλ‹¤.
- λ™μ  κ·Έλν”„ μƒμ„±μ„ μ§€μ›ν•μ—¬ λ¨λΈ κµ¬μ„±κ³Ό νλ¦„μ„ μ μ—°ν•κ² λ‹¤λ£° μ μλ‹¤.
- λ€λ¶€λ¶„μ ν…μ„ μ—°μ‚° λΌμ΄λΈλ¬λ¦¬μ™€ νΈν™λμ–΄ μ‚¬μ©ν•κΈ° νΈλ¦¬ν•λ‹¤.

### 3.2 π…Cons
- λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ΄ λ†’μ„ μ μλ‹¤.
- κ³„μ‚° κ·Έλν”„ κµ¬μ„± λ° κ·Έλλ””μ–ΈνΈ κ³„μ‚°μ— μΌλ¶€ μ¤λ²„ν—¤λ“κ°€ λ°μƒν•  μ μλ‹¤.
- μΌλ¶€ νΉμ • μ—°μ‚°μ— λ€ν• μλ™ λ―Έλ¶„μ΄ μ§€μ›λμ§€ μ•μ„ μ μλ‹¤.



_λ_