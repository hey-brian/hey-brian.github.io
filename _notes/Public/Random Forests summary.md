---
title: Random Forests summary
feed: hide
date : 2023-03-29
time : 07:13:45
tags : [summary]
format: list
comments: true
---


![](/attachments/Screenshot_2023-03-29_at_71456_AM_watermarked.jpeg)

(\### image from  [Random Forests](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.125.5395))

Breiman이 쓴 "Random Forests" 논문은 결정 트리 모델의 과적합 문제를 해결하기 위해, 랜덤한 특성 선택과 부트스트래핑을 사용하는 앙상블 방법(분산성과 안정성 강화)인 랜덤 포레스트의 개념을 제안한다.

# Key Insights and Lessons Learned
- 다수의 결정 트리 모델을 결합하여 더욱 정확한 예측을 제공한다.
- 랜덤성을 도입하여 결정 트리 모델의 과적합 문제를 해결한다.
- 간단하고 적은 수의 하이퍼파라미터를 사용하여 모델링하기 쉽다.
- 변수 중요도를 계산하여 모델의 설명력을 높인다.
- 변수의 중요도를 추정하는 데 사용될 수 있다.
- 다양한 종류의 데이터에 대해 안정적이고 강력한 예측 성능을 제공한다.
- 다른 앙상블 방법과 함께 사용될 때 더 나은 성능을 보인다.
- 다른 머신러닝 기법과 비교하여 효율적인 모델 훈련 속도를 보인다.

# Related papers
- Breiman, L. (1996). Bagging predictors.
- Friedman, J. H. (2001). Greedy function approximation: a gradient boosting machine.
- Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: data mining, inference, and prediction.
- Liu, F., & Zhou, Z. H. (2017). Ensemble learning: theoretical foundations and algorithms.

_끝_